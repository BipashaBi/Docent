# ğŸ“„ Docent: PDF Relevance Extraction Pipeline

**Docent** is a Python-based pipeline that extracts meaningful titles and structured headings from PDFs and filters them based on relevance to a given job description or persona. It leverages semantic similarity to return only the most informative content.

---

## ğŸš€ Overview

Docent processes collections of PDFs and identifies the most relevant sections using natural language understanding. The pipeline outputs a structured JSON summary that emphasizes meaningful and contextually appropriate content for any given use case (e.g., job-matching, persona targeting).

---

## ğŸ§  Pipeline Workflow

For each PDF, Docent performs the following steps:

1. **ğŸ“˜ PDF Parsing**
   Extracts hierarchical text data using `PyMuPDF`.

2. **ğŸ§¬ Embedding Generation**
   Generates sentence-level embeddings via `sentence-transformers` models (e.g., `all-MiniLM-L6-v2`).

3. **ğŸ” Semantic Similarity**
   Computes cosine similarity between PDF content and the target persona or job description.

4. **âœ‚ï¸ Subsection Filtering**
   Ranks paragraphs and retains only the most relevant ones based on similarity thresholds.

5. **ğŸ“¦ Output Formatting**
   Outputs structured data following a consistent JSON format (compatible with downstream systems or integrations).

---

## ğŸ› ï¸ Dependencies

Install required libraries using `pip`:

```bash
pip install pymupdf sentence-transformers scikit-learn numpy
```

### Required Packages

* [`sentence-transformers`](https://www.sbert.net/) â€“ Semantic embedding
* `PyMuPDF (fitz)` â€“ PDF parsing and layout detection
* `scikit-learn` â€“ Cosine similarity computation
* Standard libraries: `os`, `glob`, `json`, `re`, `numpy`

---

## ğŸ“ Project Structure

```
DOCENT/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ embedding_utils.py           # Text embedding functions
â”‚   â”œâ”€â”€ main.py                      # Entry point
â”‚   â”œâ”€â”€ output_formatter.py          # JSON output formatter
â”‚   â”œâ”€â”€ pdf_parser.py                # PDF heading extractor
â”‚   â”œâ”€â”€ pipeline.py                  # Workflow coordinator
â”‚   â”œâ”€â”€ similarity_ranking.py        # Semantic similarity scoring
â”‚   â””â”€â”€ subsection_processing.py     # Subsection refinement
â”‚
â”œâ”€â”€ Collection 1/
â”‚   â”œâ”€â”€ PDF/                         # Source PDFs
â”‚   â”œâ”€â”€ challenge1b_input.json       # Persona/Job Description input
â”‚   â””â”€â”€ challenge1b_output.json      # Final JSON output (generated)
â”‚
â”œâ”€â”€ Collection 2/
â”œâ”€â”€ Collection 3/
â”‚
â”œâ”€â”€ sentence_transformers/          # (Optional) Local model directory
â”œâ”€â”€ sentence_transformers.py        # SentenceTransformer model wrapper
â”œâ”€â”€ test_sentence_transformer.py    # Model test script
â”œâ”€â”€ output.json                     # Aggregated results (optional)
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ Dockerfile                      # Container setup
â””â”€â”€ .gitignore
```

---

## ğŸ“¥ Input

* Place PDFs inside the relevant `Collection` folder (e.g., `Collection 1/PDF/`)
* Add a `challenge1b_input.json` file in the collection folder containing the persona/job description

---

## â–¶ï¸ Running the Project

### ğŸ³ Docker (Recommended)

1. **Build Docker Image**

   ```bash
   docker build -t docent:latest .
   ```

2. **Run Container**

   ```bash
   docker run -it --rm docent:latest
   ```

> Customize paths or bind volumes if needed for local file access.

---

## ğŸ“Œ Output

* A JSON file (e.g., `challenge1b_output.json`) is generated for each collection
* Output includes:

  * Extracted titles/headings
  * Most relevant paragraphs
  * Sectional structure for better readability

---

## âœ… Summary

* ğŸ” Smart relevance-based PDF section filtering
* ğŸ’¬ Semantic understanding using transformer-based models
* ğŸ“‚ Outputs in clean, structured JSON
* ğŸ”’ Fully offline and customizable


